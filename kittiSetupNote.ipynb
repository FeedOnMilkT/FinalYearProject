{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4650ab-417a-4f72-a3bb-0201d31d2297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported and basic parameters set up ✓\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import shutil\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# KITTI dataset URLs\n",
    "KITTI_URLS = [\n",
    "    ('https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_velodyne.zip', 'data_object_velodyne.zip'),\n",
    "    ('https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_calib.zip', 'data_object_calib.zip'),\n",
    "    ('https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_label_2.zip', 'data_object_label_2.zip'),\n",
    "    ('https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_image_2.zip', 'data_object_image_2.zip')\n",
    "]\n",
    "\n",
    "# Default output paths\n",
    "DEFAULT_OUTPUT_PATH = './data/kitti'\n",
    "DEFAULT_TEMP_DIR = './data/temp/kitti'\n",
    "\n",
    "print(\"Libraries imported and basic parameters set up ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0c84571-6d3d-439c-9040-240aef2bc6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper classes and utility functions defined ✓\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Define helper classes and utility functions\n",
    "# ===================================\n",
    "class DownloadProgressBar:\n",
    "    \"\"\"Progress bar for downloads\"\"\"\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.filename = os.path.basename(url)\n",
    "        self.downloaded_bytes = 0\n",
    "        self.total_size = 0\n",
    "        self.start_time = time.time()\n",
    "        self.last_print_time = 0\n",
    "    \n",
    "    def __call__(self, count, block_size, total_size):\n",
    "        self.total_size = total_size\n",
    "        self.downloaded_bytes = count * block_size\n",
    "        \n",
    "        # Update progress every 0.5 seconds\n",
    "        current_time = time.time()\n",
    "        if current_time - self.last_print_time > 0.5:\n",
    "            self.last_print_time = current_time\n",
    "            \n",
    "            # Calculate percentage and speed\n",
    "            percent = min(100, self.downloaded_bytes * 100 // self.total_size) if self.total_size > 0 else 0\n",
    "            elapsed_time = current_time - self.start_time\n",
    "            speed = self.downloaded_bytes / (1024 * 1024 * elapsed_time) if elapsed_time > 0 else 0\n",
    "            \n",
    "            # Print progress\n",
    "            sys.stdout.write(f\"\\r{self.filename}: {percent}% | {self.downloaded_bytes/(1024*1024):.1f}MB of {self.total_size/(1024*1024):.1f}MB | {speed:.1f} MB/s\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "def download_file(url, output_path, force_download=False):\n",
    "    \"\"\"Download a file from URL to the specified output path\"\"\"\n",
    "    if os.path.exists(output_path) and not force_download:\n",
    "        logger.info(f\"File already exists at {output_path}, skipping download\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Downloading {url} to {output_path}\")\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        progress_bar = DownloadProgressBar(url)\n",
    "        urllib.request.urlretrieve(url, output_path, reporthook=progress_bar)\n",
    "        print()  # New line after progress bar\n",
    "        logger.info(f\"Successfully downloaded {url}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to download {url}: {e}\")\n",
    "        if os.path.exists(output_path):\n",
    "            os.remove(output_path)\n",
    "        raise\n",
    "\n",
    "def extract_file(input_path, output_dir, delete_after=True):\n",
    "    \"\"\"Extract a compressed file to the specified directory\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    logger.info(f\"Extracting {input_path} to {output_dir}\")\n",
    "    \n",
    "    try:\n",
    "        if input_path.endswith('.zip'):\n",
    "            with zipfile.ZipFile(input_path, 'r') as zip_ref:\n",
    "                # Get total number of files for progress tracking\n",
    "                total_files = len(zip_ref.namelist())\n",
    "                extracted_files = 0\n",
    "                \n",
    "                for file in zip_ref.namelist():\n",
    "                    zip_ref.extract(file, output_dir)\n",
    "                    extracted_files += 1\n",
    "                    if extracted_files % 100 == 0 or extracted_files == total_files:\n",
    "                        percent = min(100, extracted_files * 100 // total_files)\n",
    "                        sys.stdout.write(f\"\\rExtracting {os.path.basename(input_path)}: {percent}%\")\n",
    "                        sys.stdout.flush()\n",
    "                \n",
    "                print()  # New line after progress bar\n",
    "        else:\n",
    "            logger.error(f\"Unsupported file format: {input_path}\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"Successfully extracted {input_path}\")\n",
    "        \n",
    "        # Delete zip file after extraction if requested\n",
    "        if delete_after:\n",
    "            logger.info(f\"Deleting archive file {input_path}\")\n",
    "            os.remove(input_path)\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to extract {input_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"Helper classes and utility functions defined ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ca5a107-8d15-46f6-abf1-e52ef1ac00a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main functions defined ✓\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Define main functions\n",
    "# ===================================\n",
    "def create_kitti_structure(kitti_root):\n",
    "    \"\"\"Create the necessary directory structure for KITTI dataset\"\"\"\n",
    "    logger.info(\"Creating KITTI directory structure\")\n",
    "    \n",
    "    # Create required directories\n",
    "    directories = [\n",
    "        os.path.join(kitti_root, 'training', 'velodyne'),\n",
    "        os.path.join(kitti_root, 'training', 'calib'),\n",
    "        os.path.join(kitti_root, 'training', 'label_2'),\n",
    "        os.path.join(kitti_root, 'training', 'image_2'),\n",
    "        os.path.join(kitti_root, 'testing', 'velodyne'),\n",
    "        os.path.join(kitti_root, 'testing', 'calib'),\n",
    "        os.path.join(kitti_root, 'testing', 'image_2'),\n",
    "        os.path.join(kitti_root, 'ImageSets')\n",
    "    ]\n",
    "    \n",
    "    for directory in directories:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    logger.info(\"KITTI directory structure created\")\n",
    "\n",
    "def check_dataset_files(kitti_root):\n",
    "    \"\"\"Check if dataset files exist in the final location\"\"\"\n",
    "    logger.info(\"Checking for dataset files in the final location\")\n",
    "    \n",
    "    # Define expected directories and minimum file counts\n",
    "    expected_dirs = {\n",
    "        os.path.join(kitti_root, 'training', 'velodyne'): 10,\n",
    "        os.path.join(kitti_root, 'training', 'calib'): 10,\n",
    "        os.path.join(kitti_root, 'training', 'label_2'): 10,\n",
    "        os.path.join(kitti_root, 'training', 'image_2'): 10,\n",
    "        os.path.join(kitti_root, 'testing', 'velodyne'): 5,\n",
    "        os.path.join(kitti_root, 'testing', 'calib'): 5,\n",
    "        os.path.join(kitti_root, 'testing', 'image_2'): 5,\n",
    "        os.path.join(kitti_root, 'ImageSets'): 1\n",
    "    }\n",
    "    \n",
    "    all_valid = True\n",
    "    \n",
    "    for dir_path, min_files in expected_dirs.items():\n",
    "        if not os.path.exists(dir_path):\n",
    "            logger.warning(f\"Directory {dir_path} does not exist\")\n",
    "            all_valid = False\n",
    "            continue\n",
    "            \n",
    "        files_count = len([f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))])\n",
    "        if files_count < min_files:\n",
    "            logger.warning(f\"Directory {dir_path} contains only {files_count} files, expected at least {min_files}\")\n",
    "            all_valid = False\n",
    "        else:\n",
    "            logger.info(f\"Directory {dir_path} contains {files_count} files ✓\")\n",
    "    \n",
    "    if all_valid:\n",
    "        logger.info(\"All dataset files verified successfully ✓\")\n",
    "    else:\n",
    "        logger.warning(\"Some dataset files are missing or incomplete!\")\n",
    "    \n",
    "    return all_valid\n",
    "\n",
    "def organize_kitti_files(kitti_root, temp_dir, immediate_cleanup=True):\n",
    "    \"\"\"Organize extracted KITTI files into the proper structure\"\"\"\n",
    "    logger.info(\"Organizing KITTI files\")\n",
    "    \n",
    "    # Define source and destination folders\n",
    "    train_dirs = ['velodyne', 'calib', 'label_2', 'image_2']\n",
    "    test_dirs = ['velodyne', 'calib', 'image_2']\n",
    "    \n",
    "    # Process training data\n",
    "    for folder in train_dirs:\n",
    "        src_folder = os.path.join(temp_dir, 'training', folder)\n",
    "        dst_folder = os.path.join(kitti_root, 'training', folder)\n",
    "        \n",
    "        if os.path.exists(src_folder):\n",
    "            logger.info(f\"Moving {folder} training files\")\n",
    "            os.makedirs(dst_folder, exist_ok=True)\n",
    "            \n",
    "            file_count = len([f for f in os.listdir(src_folder) if os.path.isfile(os.path.join(src_folder, f))])\n",
    "            processed = 0\n",
    "            \n",
    "            for filename in os.listdir(src_folder):\n",
    "                src_file = os.path.join(src_folder, filename)\n",
    "                dst_file = os.path.join(dst_folder, filename)\n",
    "                \n",
    "                if os.path.isfile(src_file):\n",
    "                    shutil.copy2(src_file, dst_file)\n",
    "                    processed += 1\n",
    "                    \n",
    "                    # Print progress\n",
    "                    if processed % 100 == 0 or processed == file_count:\n",
    "                        percent = min(100, processed * 100 // file_count) if file_count > 0 else 100\n",
    "                        sys.stdout.write(f\"\\rMoving {folder} training files: {percent}%\")\n",
    "                        sys.stdout.flush()\n",
    "            \n",
    "            print()  # New line after progress\n",
    "    \n",
    "    # Process testing data\n",
    "    for folder in test_dirs:\n",
    "        src_folder = os.path.join(temp_dir, 'testing', folder)\n",
    "        dst_folder = os.path.join(kitti_root, 'testing', folder)\n",
    "        \n",
    "        if os.path.exists(src_folder):\n",
    "            logger.info(f\"Moving {folder} testing files\")\n",
    "            os.makedirs(dst_folder, exist_ok=True)\n",
    "            \n",
    "            file_count = len([f for f in os.listdir(src_folder) if os.path.isfile(os.path.join(src_folder, f))])\n",
    "            processed = 0\n",
    "            \n",
    "            for filename in os.listdir(src_folder):\n",
    "                src_file = os.path.join(src_folder, filename)\n",
    "                dst_file = os.path.join(dst_folder, filename)\n",
    "                \n",
    "                if os.path.isfile(src_file):\n",
    "                    shutil.copy2(src_file, dst_file)\n",
    "                    processed += 1\n",
    "                    \n",
    "                    # Print progress\n",
    "                    if processed % 100 == 0 or processed == file_count:\n",
    "                        percent = min(100, processed * 100 // file_count) if file_count > 0 else 100\n",
    "                        sys.stdout.write(f\"\\rMoving {folder} testing files: {percent}%\")\n",
    "                        sys.stdout.flush()\n",
    "            \n",
    "            print()  # New line after progress\n",
    "    \n",
    "    logger.info(\"KITTI files organized successfully\")\n",
    "    \n",
    "    # Verify dataset files are in place\n",
    "    check_result = check_dataset_files(kitti_root)\n",
    "    \n",
    "    # Immediately clean up temporary directory if requested\n",
    "    if immediate_cleanup and os.path.exists(temp_dir):\n",
    "        logger.info(f\"Cleaning up temporary directory {temp_dir}\")\n",
    "        shutil.rmtree(temp_dir)\n",
    "        logger.info(f\"Temporary directory {temp_dir} has been removed\")\n",
    "        \n",
    "    return check_result\n",
    "\n",
    "def create_imagesets(kitti_root):\n",
    "    \"\"\"Create train/val/test splits for KITTI if they don't already exist\"\"\"\n",
    "    logger.info(\"Checking dataset splits\")\n",
    "    \n",
    "    # Create ImageSets directory\n",
    "    imagesets_dir = os.path.join(kitti_root, 'ImageSets')\n",
    "    os.makedirs(imagesets_dir, exist_ok=True)\n",
    "    \n",
    "    # Check if split files already exist\n",
    "    if all(os.path.exists(os.path.join(imagesets_dir, f\"{split}.txt\")) for split in ['train', 'val', 'test']):\n",
    "        logger.info(\"Dataset splits already exist, preserving existing files\")\n",
    "        return\n",
    "    \n",
    "    # Get training sample IDs\n",
    "    train_velodyne_dir = os.path.join(kitti_root, 'training', 'velodyne')\n",
    "    if not os.path.exists(train_velodyne_dir):\n",
    "        logger.error(f\"Training velodyne directory {train_velodyne_dir} doesn't exist\")\n",
    "        return\n",
    "    \n",
    "    # Get sample IDs (remove file extension)\n",
    "    train_samples = [os.path.splitext(f)[0] for f in os.listdir(train_velodyne_dir) if f.endswith('.bin')]\n",
    "    train_samples.sort()\n",
    "    \n",
    "    # Get testing sample IDs\n",
    "    test_velodyne_dir = os.path.join(kitti_root, 'testing', 'velodyne')\n",
    "    test_samples = []\n",
    "    if os.path.exists(test_velodyne_dir):\n",
    "        test_samples = [os.path.splitext(f)[0] for f in os.listdir(test_velodyne_dir) if f.endswith('.bin')]\n",
    "        test_samples.sort()\n",
    "    \n",
    "    # Split training into train and val (80/20 split)\n",
    "    # Use a fixed seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    indices = np.random.permutation(len(train_samples))\n",
    "    split_idx = int(len(train_samples) * 0.8)\n",
    "    \n",
    "    train_idx = indices[:split_idx]\n",
    "    val_idx = indices[split_idx:]\n",
    "    \n",
    "    train_split = [train_samples[i] for i in train_idx]\n",
    "    val_split = [train_samples[i] for i in val_idx]\n",
    "    \n",
    "    # Sort for better readability\n",
    "    train_split.sort()\n",
    "    val_split.sort()\n",
    "    \n",
    "    # Check individual split files and only create missing ones\n",
    "    if not os.path.exists(os.path.join(imagesets_dir, 'train.txt')):\n",
    "        with open(os.path.join(imagesets_dir, 'train.txt'), 'w') as f:\n",
    "            f.write('\\n'.join(train_split))\n",
    "        logger.info(f\"Created train split with {len(train_split)} samples\")\n",
    "    else:\n",
    "        logger.info(\"Using existing train.txt file\")\n",
    "    \n",
    "    if not os.path.exists(os.path.join(imagesets_dir, 'val.txt')):\n",
    "        with open(os.path.join(imagesets_dir, 'val.txt'), 'w') as f:\n",
    "            f.write('\\n'.join(val_split))\n",
    "        logger.info(f\"Created validation split with {len(val_split)} samples\")\n",
    "    else:\n",
    "        logger.info(\"Using existing val.txt file\")\n",
    "    \n",
    "    if not os.path.exists(os.path.join(imagesets_dir, 'test.txt')):\n",
    "        with open(os.path.join(imagesets_dir, 'test.txt'), 'w') as f:\n",
    "            f.write('\\n'.join(test_samples))\n",
    "        logger.info(f\"Created test split with {len(test_samples)} samples\")\n",
    "    else:\n",
    "        logger.info(\"Using existing test.txt file\")\n",
    "\n",
    "print(\"Main functions defined ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b27ad9c7-42b1-43dd-9934-d0311d50677d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 00:37:38 - INFO - Script directory: /teamspace/studios/this_studio/FinalYearProject\n",
      "2025-03-18 00:37:38 - INFO - Setting up KITTI dataset at /teamspace/studios/this_studio/FinalYearProject/data/kitti\n",
      "2025-03-18 00:37:38 - INFO - Using temporary directory: /teamspace/studios/this_studio/FinalYearProject/data/temp/kitti\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters set up ✓\n",
      "\n",
      "Current parameters:\n",
      "- Dataset save path: /teamspace/studios/this_studio/FinalYearProject/data/kitti\n",
      "- Temporary directory: /teamspace/studios/this_studio/FinalYearProject/data/temp/kitti\n",
      "- Force re-download: False\n",
      "- Skip download: False\n",
      "- Keep temporary files: False\n",
      "- Keep archive files: False\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Set up paths and parameters\n",
    "# ===================================\n",
    "# This cell can be run multiple times to change parameter settings\n",
    "\n",
    "# User-modifiable parameters\n",
    "output_path = DEFAULT_OUTPUT_PATH  # Path to save the dataset\n",
    "temp_dir = DEFAULT_TEMP_DIR        # Temporary directory\n",
    "force_download = False             # Whether to force re-download\n",
    "skip_download = False              # Whether to skip download\n",
    "keep_temp = False                  # Whether to keep temporary files\n",
    "keep_archives = False              # Whether to keep archive files\n",
    "\n",
    "# Get script directory for reference\n",
    "try:\n",
    "    script_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "except:\n",
    "    script_dir = os.getcwd()\n",
    "\n",
    "# Handle paths - make them absolute and ensure they exist\n",
    "if output_path.startswith('./'):\n",
    "    output_path = os.path.abspath(os.path.join(script_dir, output_path))\n",
    "else:\n",
    "    output_path = os.path.abspath(output_path)\n",
    "\n",
    "if temp_dir.startswith('./'):\n",
    "    temp_dir = os.path.abspath(os.path.join(script_dir, temp_dir))\n",
    "else:\n",
    "    temp_dir = os.path.abspath(temp_dir)\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "logger.info(f\"Script directory: {script_dir}\")\n",
    "logger.info(f\"Setting up KITTI dataset at {output_path}\")\n",
    "logger.info(f\"Using temporary directory: {temp_dir}\")\n",
    "\n",
    "print(\"Parameters set up ✓\")\n",
    "print(\"\\nCurrent parameters:\")\n",
    "print(f\"- Dataset save path: {output_path}\")\n",
    "print(f\"- Temporary directory: {temp_dir}\")\n",
    "print(f\"- Force re-download: {force_download}\")\n",
    "print(f\"- Skip download: {skip_download}\")\n",
    "print(f\"- Keep temporary files: {keep_temp}\")\n",
    "print(f\"- Keep archive files: {keep_archives}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c9b8dda-63a7-4374-82c2-b1b90cf114c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 00:37:51 - INFO - Downloading https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_velodyne.zip to /teamspace/studios/this_studio/FinalYearProject/data/temp/kitti/data_object_velodyne.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_object_velodyne.zip: 9% | 2575.9MB of 27418.8MB | 29.4 MB/s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m url, filename \u001b[38;5;129;01min\u001b[39;00m KITTI_URLS:\n\u001b[1;32m      9\u001b[0m         download_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(temp_dir, filename)\n\u001b[0;32m---> 10\u001b[0m         \u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset download completed ✓\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[2], line 42\u001b[0m, in \u001b[0;36mdownload_file\u001b[0;34m(url, output_path, force_download)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     progress_bar \u001b[38;5;241m=\u001b[39m DownloadProgressBar(url)\n\u001b[0;32m---> 42\u001b[0m     \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreporthook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m()  \u001b[38;5;66;03m# New line after progress bar\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully downloaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/urllib/request.py:270\u001b[0m, in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    267\u001b[0m     reporthook(blocknum, bs, size)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     block \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block:\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/http/client.py:465\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 465\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cell 5: Download dataset (optional)\n",
    "# ===================================\n",
    "# If disk space is insufficient or download is interrupted, you can skip this step or continue later\n",
    "\n",
    "if not skip_download:\n",
    "    try:\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        for url, filename in KITTI_URLS:\n",
    "            download_path = os.path.join(temp_dir, filename)\n",
    "            download_file(url, download_path, force_download)\n",
    "        \n",
    "        print(\"Dataset download completed ✓\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during download process: {e}\")\n",
    "        print(\"You can retry later or set skip_download = True and continue with the next steps\")\n",
    "else:\n",
    "    print(\"Skipping download step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ac9862-e674-4791-8c31-d4926d72e5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Extract dataset (optional)\n",
    "# ===================================\n",
    "# If disk space is insufficient or extraction is interrupted, you can continue later\n",
    "\n",
    "if not skip_download:\n",
    "    try:\n",
    "        for _, filename in KITTI_URLS:\n",
    "            input_path = os.path.join(temp_dir, filename)\n",
    "            if os.path.exists(input_path):\n",
    "                extract_file(input_path, temp_dir, not keep_archives)\n",
    "        \n",
    "        print(\"Dataset extraction completed ✓\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during extraction process: {e}\")\n",
    "        print(\"You can retry later\")\n",
    "else:\n",
    "    print(\"Skipping extraction step\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf3d097-4e15-4a67-acd3-951e03e86303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Create directory structure\n",
    "# ===================================\n",
    "try:\n",
    "    create_kitti_structure(output_path)\n",
    "    print(\"KITTI directory structure created ✓\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating directory structure: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d070fcd-cc24-43d3-9f63-4e06613db4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 00:06:22 - INFO - Organizing KITTI files\n",
      "2025-03-16 00:06:22 - INFO - Moving velodyne testing files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving velodyne testing files: 100%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 00:09:57 - INFO - Moving calib testing files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Moving calib testing files: 100%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 00:10:37 - INFO - Moving image_2 testing files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Moving image_2 testing files: 100%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 00:12:16 - INFO - KITTI files organized successfully\n",
      "2025-03-16 00:12:16 - INFO - Checking for dataset files in the final location\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 00:12:17 - INFO - Directory /workspace/FinalYearProject/data/kitti/training/velodyne contains 7481 files ✓\n",
      "2025-03-16 00:12:18 - INFO - Directory /workspace/FinalYearProject/data/kitti/training/calib contains 7481 files ✓\n",
      "2025-03-16 00:12:19 - INFO - Directory /workspace/FinalYearProject/data/kitti/training/label_2 contains 7481 files ✓\n",
      "2025-03-16 00:12:21 - INFO - Directory /workspace/FinalYearProject/data/kitti/training/image_2 contains 7481 files ✓\n",
      "2025-03-16 00:12:22 - INFO - Directory /workspace/FinalYearProject/data/kitti/testing/velodyne contains 7518 files ✓\n",
      "2025-03-16 00:12:23 - INFO - Directory /workspace/FinalYearProject/data/kitti/testing/calib contains 7518 files ✓\n",
      "2025-03-16 00:12:25 - INFO - Directory /workspace/FinalYearProject/data/kitti/testing/image_2 contains 7518 files ✓\n",
      "2025-03-16 00:12:25 - INFO - Directory /workspace/FinalYearProject/data/kitti/ImageSets contains 3 files ✓\n",
      "2025-03-16 00:12:25 - INFO - All dataset files verified successfully ✓\n",
      "2025-03-16 00:12:25 - INFO - Cleaning up temporary directory /workspace/FinalYearProject/data/temp/kitti\n",
      "2025-03-16 00:12:32 - INFO - Temporary directory /workspace/FinalYearProject/data/temp/kitti has been removed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File organization and verification completed ✓\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Organize files and verify\n",
    "# ===================================\n",
    "try:\n",
    "    # immediate_cleanup parameter controls whether to delete temporary files immediately\n",
    "    organize_result = organize_kitti_files(output_path, temp_dir, immediate_cleanup=not keep_temp)\n",
    "    if organize_result:\n",
    "        print(\"File organization and verification completed ✓\")\n",
    "    else:\n",
    "        print(\"File organization completed, but verification found issues\")\n",
    "except Exception as e:\n",
    "    print(f\"Error organizing files: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7091372-d1b2-425f-a12f-438bf510d410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 00:12:34 - INFO - Checking dataset splits\n",
      "2025-03-16 00:12:34 - INFO - Dataset splits already exist, preserving existing files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits created ✓\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Create dataset splits\n",
    "# ===================================\n",
    "try:\n",
    "    create_imagesets(output_path)\n",
    "    print(\"Dataset splits created ✓\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating dataset splits: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c6fdac9-ad63-4a85-88e8-681ee6d315e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 00:12:37 - INFO - Checking for dataset files in the final location\n",
      "2025-03-16 00:12:38 - INFO - Directory /workspace/FinalYearProject/data/kitti/training/velodyne contains 7481 files ✓\n",
      "2025-03-16 00:12:39 - INFO - Directory /workspace/FinalYearProject/data/kitti/training/calib contains 7481 files ✓\n",
      "2025-03-16 00:12:40 - INFO - Directory /workspace/FinalYearProject/data/kitti/training/label_2 contains 7481 files ✓\n",
      "2025-03-16 00:12:42 - INFO - Directory /workspace/FinalYearProject/data/kitti/training/image_2 contains 7481 files ✓\n",
      "2025-03-16 00:12:43 - INFO - Directory /workspace/FinalYearProject/data/kitti/testing/velodyne contains 7518 files ✓\n",
      "2025-03-16 00:12:44 - INFO - Directory /workspace/FinalYearProject/data/kitti/testing/calib contains 7518 files ✓\n",
      "2025-03-16 00:12:45 - INFO - Directory /workspace/FinalYearProject/data/kitti/testing/image_2 contains 7518 files ✓\n",
      "2025-03-16 00:12:45 - INFO - Directory /workspace/FinalYearProject/data/kitti/ImageSets contains 3 files ✓\n",
      "2025-03-16 00:12:45 - INFO - All dataset files verified successfully ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "KITTI dataset setup completed successfully!\n",
      "Dataset location: /workspace/FinalYearProject/data/kitti\n",
      "===============================\n",
      "\n",
      "To use this dataset with OpenPCDet:\n",
      "1. Ensure the pcdet package is installed\n",
      "2. Use the KITTI dataset configuration in your training/testing scripts\n",
      "3. Sample command: python tools/train.py --cfg_file tools/cfgs/kitti_models/second.yaml\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Final verification\n",
    "# ===================================\n",
    "try:\n",
    "    final_check = check_dataset_files(output_path)\n",
    "    \n",
    "    print(\"===============================\")\n",
    "    if final_check:\n",
    "        print(\"KITTI dataset setup completed successfully!\")\n",
    "    else:\n",
    "        print(\"KITTI dataset setup completed with warnings!\")\n",
    "    print(f\"Dataset location: {output_path}\")\n",
    "    print(\"===============================\")\n",
    "    print(\"\\nTo use this dataset with OpenPCDet:\")\n",
    "    print(\"1. Ensure the pcdet package is installed\")\n",
    "    print(\"2. Use the KITTI dataset configuration in your training/testing scripts\")\n",
    "    print(\"3. Sample command: python tools/train.py --cfg_file tools/cfgs/kitti_models/second.yaml\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during final verification: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f802ff73-8c21-4631-a235-7e9a029433d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
